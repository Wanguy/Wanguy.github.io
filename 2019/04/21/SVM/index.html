<!DOCTYPE html>
<html lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/180.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/16.png">
  <link rel="mask-icon" href="/images/s.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"marara.xin","root":"/","scheme":"Muse","version":"7.7.2","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="简介 支持向量机与上世纪90年代正式由Vapnik和Cortes正式发表，在文本分类任务中显示 出卓越性能，很快成为机器学习主流技术，直接掀起了2000年前后统计学习的高潮。 支持向量机思想直观，但细节复杂，涵盖凸优化，核函数，拉格朗日算子等理论 支持向量机 （support vector machine）：  二分类模型，也可用于回归分析 学习策略： 最大间隔化  凸优化，对偶，KKT条件">
<meta name="keywords" content="Statistics,Machine Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="支持向量机（SVM）">
<meta property="og:url" content="https://marara.xin/2019/04/21/SVM/index.html">
<meta property="og:site_name" content="Gavin&#39;s Blog">
<meta property="og:description" content="简介 支持向量机与上世纪90年代正式由Vapnik和Cortes正式发表，在文本分类任务中显示 出卓越性能，很快成为机器学习主流技术，直接掀起了2000年前后统计学习的高潮。 支持向量机思想直观，但细节复杂，涵盖凸优化，核函数，拉格朗日算子等理论 支持向量机 （support vector machine）：  二分类模型，也可用于回归分析 学习策略： 最大间隔化  凸优化，对偶，KKT条件">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc35afcaf15.png">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc35bd761f4.png">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc35afd510f.png">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc35bdb1598.png">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc35afb6756.png">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc35afcbb05.png">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc35bd8e91c.png">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc35bd8e91c.png">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc35afdc5c1.png">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc35afb67ad.png">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc35afe1cf9.png">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc35afe1cf9.png">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc35afdce29.png">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc35afb8ef6.png">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc310c18910.png">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc310bf2d06.png">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc310bf38ed.png">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc310c26621.png">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc310c26fd3.png">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc310c31cb7.png">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc310c1a556.png">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc310c41c4c.png">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc310bd8e38.png">
<meta property="og:image" content="https://i.loli.net/2019/04/21/5cbc310c1e34e.png">
<meta property="og:updated_time" content="2019-04-22T01:41:28.161Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="支持向量机（SVM）">
<meta name="twitter:description" content="简介 支持向量机与上世纪90年代正式由Vapnik和Cortes正式发表，在文本分类任务中显示 出卓越性能，很快成为机器学习主流技术，直接掀起了2000年前后统计学习的高潮。 支持向量机思想直观，但细节复杂，涵盖凸优化，核函数，拉格朗日算子等理论 支持向量机 （support vector machine）：  二分类模型，也可用于回归分析 学习策略： 最大间隔化  凸优化，对偶，KKT条件">
<meta name="twitter:image" content="https://i.loli.net/2019/04/21/5cbc35afcaf15.png">

<link rel="canonical" href="https://marara.xin/2019/04/21/SVM/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true
  };
</script>

  <title>支持向量机（SVM） | Gavin's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Gavin's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">分享有趣</h1>
      
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags<span class="badge">16</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories<span class="badge">2</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives<span class="badge">23</span></a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="search-pop-overlay">
  <div class="popup search-popup">
      <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

  </div>
</div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/Wanguy" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="https://marara.xin/2019/04/21/SVM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/IMG_5946.png">
      <meta itemprop="name" content="Gavin">
      <meta itemprop="description" content="Make it, Fake it.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Gavin's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          支持向量机（SVM）
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 04-21-2019 16:39:10" itemprop="dateCreated datePublished" datetime="2019-04-21T16:39:10+08:00">04-21-2019</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 04-22-2019 09:41:28" itemprop="dateModified" datetime="2019-04-22T09:41:28+08:00">04-22-2019</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Note/" itemprop="url" rel="index"><span itemprop="name">Note</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><ul>
<li>支持向量机与上世纪90年代正式由Vapnik和Cortes正式发表，在文本分类任务中显示 出卓越性能，很快成为机器学习主流技术，直接掀起了2000年前后统计学习的高潮。</li>
<li>支持向量机思想直观，但细节复杂，涵盖凸优化，核函数，拉格朗日算子等理论</li>
<li>支持向量机 （support vector machine）： <ul>
<li>二分类模型，也可用于回归分析</li>
<li>学习策略： 最大间隔化 </li>
<li>凸优化，对偶，KKT条件</li>
</ul>
</li>
</ul>
<a id="more"></a>
<h2 id="机器学习算法分类"><a href="#机器学习算法分类" class="headerlink" title="机器学习算法分类"></a>机器学习算法分类</h2><ul>
<li><p>回归（监督学习）：</p>
<ul>
<li>线性回归 （linear regression） </li>
<li>决策树回归 （decision tree regression）</li>
<li><span style="color:red">支持向量回归 （support vector regression）<span></span></span></li>
<li>神经网络 （neural network）</li>
</ul>
</li>
<li>分类 （监督学习）：<ul>
<li>逻辑回归 （logistic regression）</li>
<li>决策树分类（decision tree classification） </li>
<li><span style="color:red">支持向量机 （support vector machine）<span></span></span></li>
<li>朴素贝叶斯 （naive Bayes） </li>
</ul>
</li>
<li>聚类（半监督学习）： <ul>
<li>K 均值 （K-means）</li>
</ul>
</li>
</ul>
<h2 id="线性分类"><a href="#线性分类" class="headerlink" title="线性分类"></a>线性分类</h2><p>分类算法的起源： 逻辑回归</p>
<script type="math/tex; mode=display">
h_{\theta}(x)=\frac{1}{1+e^{-f(x)}}</script><script type="math/tex; mode=display">
P(y=1 | x ; \theta)=h_{\theta}(x)</script><script type="math/tex; mode=display">
P(y=0 | x ; \theta)=1-h_{\theta}(x)</script><p><img src="https://i.loli.net/2019/04/21/5cbc35afcaf15.png" width="500"></p>
<script type="math/tex; mode=display">
f(x)=w^{T} x+b</script><p>$𝑤$: 权重 </p>
<p>$𝑏$: 截距（位移）</p>
<h3 id="如何决定最好的参数：-𝑤-amp-𝑏"><a href="#如何决定最好的参数：-𝑤-amp-𝑏" class="headerlink" title="如何决定最好的参数：$𝑤$ &amp; $𝑏$"></a>如何决定最好的参数：$𝑤$ &amp; $𝑏$</h3><p><img src="https://i.loli.net/2019/04/21/5cbc35bd761f4.png" width="300"></p>
<blockquote>
<p>支持向量机的核心思想： 最大间隔化， 最不受到噪声的干扰</p>
</blockquote>
<ul>
<li><p>SVM划分的超平面：</p>
<script type="math/tex; mode=display">
w^{T} x+b=0</script><p>$\boldsymbol{w}=\left(w_{1}, w_{2}, \dots, w_{d}\right)$为法向量，决定超平面方向</p>
</li>
<li><p>假设超平面将样本正确划分</p>
</li>
<li><script type="math/tex; mode=display">
\left\{\begin{array}{l}{w^{T} x+b \geq 1, y=+1} \\ {w^{T} x+b \leq-1, y=-1}\end{array}\right.</script></li>
</ul>
<ul>
<li>距离超平面最近的几个点叫做支持向量</li>
<li>间隔（margin）：$r=\frac{2}{| | w| |}$</li>
</ul>
<p><img src="https://i.loli.net/2019/04/21/5cbc35afd510f.png" width="500"></p>
<ul>
<li>支持向量机： 最大间隔化</li>
<li>约束条件:</li>
</ul>
<script type="math/tex; mode=display">
\begin{array}{c}{\max _{w, b} \frac{2}{\|w\|}} \\ {\text { s.t. } y_{i}\left(w^{T} x_{i}+b\right) \geq 1}\end{array}\\
\left(x_{i}, y_{i}\right) \in D 属于样本空间</script><p>​    等价于</p>
<script type="math/tex; mode=display">
\begin{array}{c}{\min _{w, b} \frac{1}{2}\|w\|^{2}} \\ {\text { s.t. } y_{i}\left(w^{T} x_{i}+b\right) \geq 1}\end{array}\\
\left(x_{i}, y_{i}\right) \in D 属于样本空间</script><p>支持向量机的最基本表达形式， 是一个线性约束的凸二次规划问题， 有最优解</p>
<script type="math/tex; mode=display">
D=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \ldots,\left(x_{n}, y_{n}\right)\right\}, y_{i} \in\{-1,+1\}</script><h3 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h3><p>凸函数：设$ f(x)$ 为定义在$n$维欧式空间中某个凸集 $S$ 上的函数，若对于任</p>
<p>何实数$𝛼 0 &lt; 𝛼 &lt; 1 $以及 $S$ 中的不同两点 $x$ , $y$ , 均有：</p>
<script type="math/tex; mode=display">
f\left((1-\alpha) x_{0}+\alpha x_{1}\right) \leq(1-\alpha) f\left(x_{0}\right)+\alpha f\left(x_{1}\right)</script><p>那么， $𝑓(𝑥)$ 为定义在凸集 $S$ 上的凸函数</p>
<p><img src="https://i.loli.net/2019/04/21/5cbc35bdb1598.png" width="500"></p>
<p><img src="https://i.loli.net/2019/04/21/5cbc35afb6756.png" width="500"></p>
<blockquote>
<p>左边为凸函数，右边为非凸函数</p>
</blockquote>
<h3 id="凸优化"><a href="#凸优化" class="headerlink" title="凸优化"></a>凸优化</h3><p>常见的凸函数：</p>
<ul>
<li>仿射函数（affine function）：$w^{T} x+b$</li>
<li>二次函数（quadratic function）：$x^{T} A x+b x+c$（$A$ 为半正定矩阵）</li>
<li>最小平方差函数（least square）：$| | y-A x| |_{2}^{2}$（$A ^T A$ 总是半正定矩阵）</li>
<li>max 函数：$\max \left(x_{1}, x_{2}, \ldots, x_{n}\right)$</li>
</ul>
<p>有约束条件的凸优化问题：</p>
<script type="math/tex; mode=display">
\begin{array}{c}{\min _{x} f(x)} \\ {\text { s.t. } g_{i}(x) \leq 0, i=1,2, \ldots, m} \\ {h_{j}(x)=0, j=1,2, \dots, n}\end{array}</script><p>如果 $f(x)$, $g(x)$ 为凸函数，$h(x)$ 为仿射函数时， 这是一个凸优化的问题。</p>
<p>对于支持向量机：</p>
<script type="math/tex; mode=display">
\begin{array}{c}{\min _{w, b} \frac{1}{2}| | w| |^{2}} \\ {\text {s.t.} y_{i}\left(w^{T} x_{i}+b\right) \geq 1}\end{array}</script><p>SVM 是一个凸二次规划问题， 有最优解</p>
<h2 id="拉格朗日对偶"><a href="#拉格朗日对偶" class="headerlink" title="拉格朗日对偶"></a>拉格朗日对偶</h2><p>拉格朗日乘子法（Lagrange multipliers）</p>
<ul>
<li>一种寻找多元函数在一组约束条件下的极值的方法。通过引入拉格朗日乘子，可将有 $d$ 个变量和 $k$ 个约束条件的最优化问题转化为 $d+k$ 个变量的无约束优化问题求解。</li>
<li>将原问题的约束规划问题转化为对偶问题，易于求解</li>
</ul>
<h3 id="拉格朗日乘子：等式约束"><a href="#拉格朗日乘子：等式约束" class="headerlink" title="拉格朗日乘子：等式约束"></a>拉格朗日乘子：等式约束</h3><p><img src="https://i.loli.net/2019/04/21/5cbc35afcbb05.png" width="300"></p>
<p>对于等式约束问题：</p>
<script type="math/tex; mode=display">
\begin{array}{c}{\min f(x)} \\ {\text {s.t.} g_{i}(x)=0}\end{array}</script><p>那么必然有：</p>
<ol>
<li>约束曲面上的任一点 $x$, 该点的梯度$\nabla𝑔( 𝑥)$ 正交于约束曲面</li>
<li>在最优点$𝑥 ^∗$ ,$\nabla𝑓 (𝑥)^ ∗$ 也正交于曲面</li>
</ol>
<p>则存在$\lambda \neq 0$ :</p>
<script type="math/tex; mode=display">
\nabla f\left(x^{*}\right)+\lambda \nabla g\left(x^{*}\right)=0</script><p>可定义拉格朗日函数, 𝜆 为拉格朗日乘子：</p>
<script type="math/tex; mode=display">
L(x, \lambda)=f(x)+\lambda g(x)</script><p>同时：</p>
<script type="math/tex; mode=display">
\nabla_{x} L(x, \lambda)=0 ; \quad \nabla_{\lambda} L(x, \lambda)=0</script><h3 id="拉格朗日乘子：不等式约束"><a href="#拉格朗日乘子：不等式约束" class="headerlink" title="拉格朗日乘子：不等式约束"></a>拉格朗日乘子：不等式约束</h3><p><img src="https://i.loli.net/2019/04/21/5cbc35bd8e91c.png" width="300"></p>
<p>对于不等式约束问题：</p>
<script type="math/tex; mode=display">
\begin{array}{c}{\min f(x)} \\ {\text { s.t. } g_{i}(x) \leq 0}\end{array}</script><ol>
<li>$g(x)&lt;0$，约束不起作用，直接求解 $\nabla f(x)=0, \lambda=0$</li>
<li>$g(x)=0, \nabla f\left(x^{<em>}\right)+\lambda \nabla g\left(x^{</em>}\right)=0, \lambda&gt;0$<br>那么拉格朗日函数为：<script type="math/tex; mode=display">
L(x, \lambda)=f(x)+\lambda g(x)</script>同时需要满足的条件为：<script type="math/tex; mode=display">
\begin{array}{c}{\nabla_{x} L(x, \lambda)=0} \\ {\nabla_{\lambda} L(x, \lambda)=0} \\ {\lambda g(x)=0} \\ {\lambda \geq 0}\end{array}\\
g(x) \leq 0</script></li>
</ol>
<p>即Karush-Kuhn-Tucker (KKT) 条件</p>
<h3 id="拉格朗日对偶-1"><a href="#拉格朗日对偶-1" class="headerlink" title="拉格朗日对偶"></a>拉格朗日对偶</h3><p>广义拉格朗日函数：</p>
<script type="math/tex; mode=display">
\begin{array}{c}{\min _{x} f(x)} \\ {\text { s.t. } g_{i}(x) \leq 0, i=1,2, \ldots, n} \\ {h_{j}(x)=0, j=1,2, \ldots, m}\end{array}</script><script type="math/tex; mode=display">
L(x, u, \lambda)=f(x)+\sum_{i=1}^{n} \mu_{i} g_{i}(x)+\sum_{i=1}^{m} \lambda_{j} h_{j}(x)</script><p>满足 KKT 条件：</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{c}{\nabla_{x, \mu, \lambda} L(x, \mu, \lambda)=0} 
\\ {g_{i}(x) \leq 0} 
\\ {\mu_{i} \geq 0} 
\\ {\mu_{i} g_{i}(x)=0}\end{array}\right.</script><p>构造拉格朗日乘子：</p>
<script type="math/tex; mode=display">
\begin{array}{c}{\min _{x} f(x)} \\ {\text { s.t. } g_{i}(x) \leq 0, i=1,2, \ldots, n} \\ {h_{j}(x)=0, j=1,2, \ldots, m}\end{array}</script><script type="math/tex; mode=display">
L(x, u, \lambda)=f(x)+\sum_{i=1}^{n} \mu_{i} g_{i}(x)+\sum_{i=1}^{m} \lambda_{j} h_{j}(x)\left(\mu_{i} \geq 0\right)</script><p>主问题（primal problem）：$p^{*}=\min _{x} \max _{\mu, \lambda} L(x, \mu, \lambda)$</p>
<p>其对偶问题（dual problem）：$d^{*}=\max _{\mu, \lambda} \min _{x} L(x, \mu, \lambda)$</p>
<p>通常：$d^{<em>} \leq p^{</em>}$ 即对偶问题的最优解是原始主问题最优解的下限（弱对偶性）</p>
<p>但若满足KKT 条件的方程组的解：</p>
<script type="math/tex; mode=display">
\begin{array}{l}{\nabla_{x, \mu, \lambda} L(x, \mu, \lambda)=0} \\ {g_{i}(x) \leq 0} \\ {\mu_{i} \geq 0} \\ {\mu_{i} g_{i}(x)=0}\end{array}</script><p>此时 $𝑑 ^∗ = 𝑝 ^∗$ 强对偶性，原始问题和对偶问题最优解严格相等</p>
<blockquote>
<p>支持向量机：利用拉格朗日对偶和KKT条件求解的经典对偶问题</p>
</blockquote>
<h3 id="支持向量机的对偶问题"><a href="#支持向量机的对偶问题" class="headerlink" title="支持向量机的对偶问题"></a>支持向量机的对偶问题</h3><p>支持向量机最大间隔化下的损失函数：</p>
<script type="math/tex; mode=display">
\begin{array}{c}{\min _{w, b} \frac{1}{2}| | w| |^{2}} \\ {\text { s.t. } y_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right) \geq 1, i=1,2,3, \ldots, m}\end{array}</script><p>那么约束可以写为：</p>
<script type="math/tex; mode=display">
g_{i}(\boldsymbol{w})=1-y_{i}\left(\boldsymbol{w}^{T} x_{i}+b\right) \leq 0</script><p>构造拉格朗日函数：</p>
<script type="math/tex; mode=display">
L(\boldsymbol{w}, b, \boldsymbol{\lambda})=\frac{1}{2}| | \boldsymbol{w}| |^{2}+\sum_{i=1}^{m} \lambda_{i}\left(1-y_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{\boldsymbol{i}}+b\right)\right)</script><p>找到对偶问题的形式：</p>
<script type="math/tex; mode=display">
\nabla_{w} L(\boldsymbol{w}, b, \boldsymbol{\lambda})=0 \Rightarrow \boldsymbol{w}=\sum_{i=1}^{m} \lambda_{i} y_{i} \boldsymbol{x}_{i}</script><script type="math/tex; mode=display">
\nabla_{b} L(\boldsymbol{w}, b, \lambda)=0 \Rightarrow 0=\sum_{i=1}^{m} \lambda_{i} y_{i}</script><p><img src="https://i.loli.net/2019/04/21/5cbc35bd8e91c.png" width="500"></p>
<p>那么代入拉格朗日函数可以得到其对偶问题：</p>
<script type="math/tex; mode=display">
\max _{\lambda} W(\lambda)=\max _{\lambda} \min _{w, b} L(w, b, \lambda)=\sum_{i=1}^{m} \lambda_{i}-\frac{1}{2} \sum_{i=1}^{m} y_{i} y_{j} \lambda_{i} \lambda_{j} x_{i}^{T} x_{j}\\
\begin{array}{c}{\text { s.t. } \sum_{i=1}^{m} \lambda_{i} y_{i}=0} \\ {\lambda_{i} \geq 0, i=1,2, \ldots, m}\end{array}</script><p>上述过程满足KKT条件：</p>
<script type="math/tex; mode=display">
\begin{array}{c}{\lambda_{i} \geq 0} \\ {1-y_{i} f\left(x_{i}\right) \leq 0} \\ {\lambda_{i}\left(1-y_{i} f\left(x_{i}\right)\right)=0}\end{array}</script><script type="math/tex; mode=display">
f(\boldsymbol{x})=\boldsymbol{w}^{T} \boldsymbol{x}+b=\sum_{i=1}^{m} \lambda_{i} y_{i} \boldsymbol{x}_{\boldsymbol{i}}^{T} \boldsymbol{x}+b</script><script type="math/tex; mode=display">
\begin{array}{c}{\lambda_{i} \geq 0} \\ {1-y_{i} f\left(x_{i}\right) \leq 0} \\ {\lambda_{i}\left(1-y_{i} f\left(x_{i}\right)\right)=0}\end{array}</script><p>如果$ y_{i} f\left(x_{i}\right)&gt;1$, 那么$𝜆 _i = 0$, 对于$𝑓(𝑥)$ 没有贡献</p>
<h2 id="核函数（Kernel）"><a href="#核函数（Kernel）" class="headerlink" title="核函数（Kernel）"></a>核函数（Kernel）</h2><p>假设有个映射函数从二维空间映射到三维空间：</p>
<script type="math/tex; mode=display">
\Phi : \left( \begin{array}{l}{x_{1}} \\ {x_{2}}\end{array}\right) \rightarrow \left( \begin{array}{c}{x_{1}^{2}} \\ {x_{2}^{2}} \\ {\sqrt{2} x_{1} x_{2}}\end{array}\right) \quad \mathbb{R}^{2} \rightarrow \mathbb{R}^{3}</script><p><img src="https://i.loli.net/2019/04/21/5cbc35afdc5c1.png" width="800"></p>
<p>优化问题变为：</p>
<script type="math/tex; mode=display">
\begin{array}{c}{\min _{w, b} \frac{1}{2}\|w\|^{2}} \\ {\text {s.t.} y_{i}\left(w^{T} \phi\left(x_{i}\right)+b\right) \geq 1}\end{array}</script><p>对偶问题为：</p>
<script type="math/tex; mode=display">
\max _{\lambda} W(\lambda)=\sum_{i=1}^{m} \lambda_{i}-\frac{1}{2} \sum_{i=1}^{m} y_{i} y_{j} \lambda_{i} \lambda_{j} \phi\left(x_{i}\right)^{T} \phi\left(x_{j}\right)\\
\begin{array}{c}{\text { s.t. } \sum_{i=1}^{m} \lambda_{i} y_{i}=0} \\ {\lambda_{i} \geq 0, i=1,2, \dots, m}\end{array}</script><p><img src="https://i.loli.net/2019/04/21/5cbc35afb67ad.png" width="800"></p>
<p>单独计算映射函数$\phi(x)$以及$\phi\left(x_{i}\right)^{T} \phi\left(x_{j}\right)$十分困难，现设想构造核函数</p>
<p>比如：</p>
<script type="math/tex; mode=display">
\Phi : \left( \begin{array}{l}{x_{1}} \\ {x_{2}}\end{array}\right) \rightarrow \left( \begin{array}{c}{x_{1}^{2}} \\ {x_{2}^{2}} \\ {\sqrt{2} x_{1} x_{2}}\end{array}\right) \mathbb{R}^{2} \rightarrow \mathbb{R}^{3}</script><script type="math/tex; mode=display">
\begin{align}
\Phi(x)^{T} \Phi(z)&=\left(x_{1}^{2}, x_{2}^{2}, \sqrt{2} x_{1} x_{2}\right) \left( \begin{array}{c}{z_{1}^{2}} \\ {z_{2}^{2}} \\ {\sqrt{2} z_{1} z_{2}}\end{array}\right)\\
&=x_{1}^{2} z_{1}^{2}+x_{1}^{2} z_{1}^{2}+2 x_{1} z_{1} x_{1} z_{1}\\
&=\left(x_{1} z_{1}+x_{2} z_{2}\right)^{2}\\ 
&=\left(\boldsymbol{x}^{T} \boldsymbol{z}\right)^{2}
\end{align}</script><script type="math/tex; mode=display">
k\left(x_{i}, x_{j}\right)=\phi\left(x_{i}\right)^{T} \phi\left(x_{j}\right)</script><ul>
<li>训练器不用学习和计算映射函数的显性表达式</li>
<li>寻找到核函数，然后在原始样本空间做内积，比如 $k(x, z)=\left(x^{T} z\right)^{2}$</li>
</ul>
<h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>线性核函数：$k\left(x_{i}, x_{j}\right)=x_{i}^{T} x_{j}$</p>
<p>多项式核函数：$k\left(x_{i}, x_{j}\right)=\left(x_{i}^{T} x_{j}\right)^{d}\quad d&gt;0$</p>
<p>高斯核函数：$k\left(x_{i}, x_{j}\right)=e^{\left(-\frac{\left|x_{i}-x_{i}\right|^{2}}{2 \sigma^{2}}\right)} \quad \sigma&gt;0$</p>
<p>拉普拉斯核函数：$k\left(x_{i}, x_{j}\right)=e^{\left(-\frac{\left|x_{i}-x_{j}\right|}{2 \sigma^{2}}\right)} \quad \sigma&gt;0$</p>
<p>Sigmoid 核函数：$k\left(x_{i}, x_{j}\right)=\tanh \left(\beta x_{i}^{T} x_{j}+\theta\right) \quad \beta&gt;0, \theta&lt;0$</p>
<h3 id="正则化与软间隔"><a href="#正则化与软间隔" class="headerlink" title="正则化与软间隔"></a>正则化与软间隔</h3><p>针对情况： 如果样本不是完全能够划分开</p>
<p>解决方法： 允许支持向量机在一些样本出错，定义软间隔</p>
<p>引入正则化强度参数$C$，损失函数重新定义为：</p>
<script type="math/tex; mode=display">
\min _{w, b} \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{m} \max \left(0,1-y_{i} f\left(x_{i}\right)\right)</script><p>引入松弛变量（slack variables）$\xi_{i} \geq 0$：</p>
<script type="math/tex; mode=display">
\begin{array}{c}{\min _{w, b, \xi} \frac{1}{2}| | w| |^{2}+C \sum_{i=1}^{m} \xi_{i}} \\ {\text {s.t.} y_{i} f\left(x_{i}\right) \geq 1-\xi_{i}} \\ {\xi_{i} \geq 0, i=1,2,3 \ldots m}\end{array}</script><p><img src="https://i.loli.net/2019/04/21/5cbc35afe1cf9.png" width="300"></p>
<h3 id="软间隔支持向量机"><a href="#软间隔支持向量机" class="headerlink" title="软间隔支持向量机"></a>软间隔支持向量机</h3><p>拉格朗日函数：</p>
<script type="math/tex; mode=display">
L(\boldsymbol{w}, b, \boldsymbol{\xi}, \boldsymbol{\lambda}, \boldsymbol{r})=\frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m} \xi_{i}+\sum_{i=1}^{m} \lambda_{i}\left(1-\xi_{i}-y_{i}\left(\boldsymbol{w}^{T} \boldsymbol{x}_{i}+b\right)\right)-\sum_{i=1}^{m} r_{i} \xi_{i}\left(\lambda_{i} \geq 0, r_{i} \geq 0\right)</script><p>对$𝑤, 𝑏, \xi$求偏导可得：</p>
<script type="math/tex; mode=display">
0=\sum_{i=1}^{m} \lambda_{i} y_{i} \quad w=\sum_{i=1}^{m} \lambda_{i} y_{i} x_{i} \quad C=\lambda_{i}+r_{i}</script><p>相应的对偶问题为：</p>
<script type="math/tex; mode=display">
\begin{aligned} \max _{\lambda} W(\lambda)=& \sum_{i=1}^{m} \lambda_{i}-\frac{1}{2} \sum_{i=1}^{m} y_{i} y_{j} \lambda_{i} \lambda_{j} x_{i}^{T} x_{j} \\ & \text { s.t. } \sum_{i=1}^{m} \lambda_{i} y_{i}=0 \\ C \geq \lambda_{i} & \geq 0, i=1,2, \ldots, m \end{aligned}</script><p>软间隔的KKT条件为：</p>
<script type="math/tex; mode=display">
\begin{array}{c}{\lambda_{i} \geq 0, r_{i} \geq 0} \\ {1-y_{i} f\left(x_{i}\right)-\xi_{i} \leq 0}\end{array}\\
\begin{array}{c}{\lambda_{i}\left(1-y_{i} f\left(x_{i}\right)-\xi_{i}\right)=0} \\ {\xi_{i} \geq 0, r_{i} \xi_{i}=0}\end{array}</script><p>支持向量：</p>
<script type="math/tex; mode=display">
\begin{array}
{llll}{\lambda_{i}=0} & {\Rightarrow y_{i} f\left(x_{i}\right) \geq 1}\quad样本落在间隔外，不影响模型 \\
{0<\lambda_{i}<C} & {\Rightarrow y_{i} f\left(x_{i}\right)=1}\quad样本落在最大间隔边界上，为支持向量\\
{\lambda_{i}=C, \xi_{i} \leq 1}& {\Rightarrow y_{i} f\left(x_{i}\right)=1-\xi_{i} \leq 1}\quad样本落在间隔内部为支持向量\\
{\lambda_{i}=C, \xi_{i}>1}& {\Rightarrow y_{i} f\left(x_{i}\right)=1-\xi_{i}<0}\quad样本被错误分类，为支持向量
\end{array}</script><h2 id="支持向量机分类与回归"><a href="#支持向量机分类与回归" class="headerlink" title="支持向量机分类与回归"></a>支持向量机分类与回归</h2><p><img src="https://i.loli.net/2019/04/21/5cbc35afe1cf9.png" width="300"></p>
<p>对于SVM分类，容忍一定数目的错分类</p>
<p><img src="https://i.loli.net/2019/04/21/5cbc35afdce29.png" width="300"></p>
<p>对于SVM回归，容忍 $\varepsilon$ 以内的偏差，margin以外的计算损失</p>
<h3 id="支持向量机回归分析（SVR）"><a href="#支持向量机回归分析（SVR）" class="headerlink" title="支持向量机回归分析（SVR）"></a>支持向量机回归分析（SVR）</h3><p>SVR的主凸优化问题：</p>
<script type="math/tex; mode=display">
\begin{array}{c}{\min _{w} \frac{1}{2}| | w| |^{2}} \\ {\text { s.t. } y_{i}-w^{T} x_{i}-b \leq \epsilon} \\ {w^{T} x_{i}+b-y_{i} \leq \epsilon}\end{array}</script><p><img src="https://i.loli.net/2019/04/21/5cbc35afb8ef6.png" width="300"></p>
<p>软间隔，对于落在margin以外的点计入损失：</p>
<script type="math/tex; mode=display">
\min _{w} \frac{1}{2}| | w| |^{2}+C \sum_{i=1}^{m}\left(\xi_{i}+\xi_{i}^{\prime}\right)\\
\begin{array}{c}{\text {s.t.w}^{T} x_{i}+b-y_{i} \leq \epsilon+\xi_{i}} \\ {y_{i}-w^{T} x_{i}-b \leq \epsilon+\xi_{i}^{\prime}} \\ {\xi_{i} \geq 0, \xi_{i}^{\prime} \geq 0, i=1,2,3 \dots m}\end{array}</script><h3 id="SVR-对偶问题"><a href="#SVR-对偶问题" class="headerlink" title="SVR 对偶问题"></a>SVR 对偶问题</h3><p>构造拉格朗日函数, 引入拉格朗日乘子 $\lambda_{i} \geq 0, \lambda_{i}^{\prime} \geq 0, r_{i} \geq 0, r_{i}^{\prime} \geq 0$</p>
<script type="math/tex; mode=display">
L\left(\boldsymbol{w}, b, \boldsymbol{\lambda}, \boldsymbol{\lambda}^{\prime}, \boldsymbol{\xi}, \boldsymbol{\xi}^{\prime}, \boldsymbol{r}, \boldsymbol{r}^{\prime}\right)=\frac{1}{2}\|\boldsymbol{w}\|^{2}+C \sum_{i=1}^{m}\left(\xi_{i}+\xi_{i}^{\prime}\right)+\sum_{i=1}^{m} \lambda_{i}\left(f\left(x_{i}\right)-y_{i}-\epsilon-\xi_{i}\right)\\
+\sum_{i=1}^{m} \lambda_{i}^{\prime}\left(y_{i}-f\left(x_{i}\right)-\epsilon-\xi_{i}^{\prime}\right)-\sum_{i=1}^{m} r_{i} \xi_{i}-\sum_{i=1}^{m} r_{i}^{\prime} \xi_{i}^{\prime}</script><p>对 $w, b, \xi, \xi$ 求偏导则有：</p>
<script type="math/tex; mode=display">
w=\sum_{i=1}^{m}\left(\lambda_{i}^{\prime}-\lambda_{i}\right) x_{i} \quad 0=\sum_{i=1}^{m}\left(\lambda_{i}-\lambda_{i}^{\prime}\right) \quad C=\lambda_{i}+r_{i} \quad C=\lambda_{i}^{\prime}+r_{i}^{\prime}</script><p>SVR 对偶形式为：</p>
<script type="math/tex; mode=display">
\max _{\lambda, \lambda^{\prime}} W\left(\lambda, \lambda^{\prime}\right)=\sum_{i=1}^{m}\left[y_{i}\left(\lambda_{i}^{\prime}-\lambda_{i}\right)-\epsilon\left(\lambda_{i}^{\prime}+\lambda_{i}\right)\right]-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m}\left(\lambda_{i}^{\prime}-\lambda_{i}\right)\left(\lambda_{j}^{\prime}-\lambda_{j}\right) x_{i}^{T} x_{j}\\
\begin{array}{c}{\text { s.t. } \sum_{i=1}^{m}\left(\lambda_{i}^{\prime}-\lambda_{i}\right)=0} \\ {C \geq \lambda_{i}, \lambda_{i}^{\prime} \geq 0, i=1,2, \ldots, m}\end{array}</script><p>满足KKT条件，则要求：</p>
<script type="math/tex; mode=display">
\lambda_{i} \lambda_{i}^{\prime}=0, \xi_{i} \xi_{i}^{\prime}=0\\
\begin{aligned} \lambda_{i}\left(f\left(x_{i}\right)-y_{i}-\epsilon-\xi_{i}\right) &=0 \\ \lambda_{i}^{\prime}\left(y_{i}-f\left(x_{i}\right)-\epsilon-\xi_{i}^{\prime}\right) &=0 \end{aligned}\\
\left(C-\lambda_{i}\right) \xi_{i}=0,\left(C-\lambda_{i}^{\prime}\right) \xi_{i}^{\prime}=0</script><p>当且仅当：</p>
<script type="math/tex; mode=display">
Where \quad f\left(x_{i}\right)-y_{i}-\epsilon-\xi_{i}=0 \mathrm, \quad \lambda_{i} \neq 0\\
Where  \quad y_{i}-f\left(x_{i}\right)-\epsilon-\xi_{i}^{\prime}=0 \mathrm, \quad \lambda_{i}^{\prime} \neq 0</script><p>也就是当样本落在 $\epsilon$ −间隔带以外，为支持向量，相应的 $\lambda_{i}$，$\lambda_{i}^{\prime}$ 取非零值</p>
<script type="math/tex; mode=display">
f(\boldsymbol{x})=\sum_{i=1}^{m}\left(\lambda_{i}^{\prime}-\lambda_{i}\right) \boldsymbol{x}_{\boldsymbol{i}}^{\boldsymbol{T}} \boldsymbol{x}+\boldsymbol{b}</script><p>对于 $0&lt;\lambda_{i}&lt;C, \xi_{i}=0$ ，取 $b=y_{i}+\epsilon-\sum_{i=1}^{m}\left(\lambda_{i}^{\prime}-\lambda_{i}\right) x_{i}^{T} x$ 的平均值</p>
<h2 id="支持向量机算法总结"><a href="#支持向量机算法总结" class="headerlink" title="支持向量机算法总结"></a>支持向量机算法总结</h2><ul>
<li>优点：<ul>
<li>解决高维特征的分类问题和回归问题很有效，在特征维度大于样本数时依然有很好的效果  </li>
<li>稀疏性：仅仅使用支持向量来做超平面的决策，无需依赖全部数据 </li>
<li>核函数可以很灵活的来解决各种非线性的分类回归问题 </li>
<li>样本量不是海量数据的时候，分类准确率高，泛化能力强</li>
</ul>
</li>
<li>缺点：<ul>
<li>如果特征维度远远大于样本数，则SVM表现一般 </li>
<li>SVM在样本量非常大，核函数映射维度非常高时，计算量过大，不太适合使用 </li>
<li>SVM对缺失数据敏感</li>
</ul>
</li>
</ul>
<h2 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h2><p><code>In[1]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sklearn <span class="keyword">as</span> sk</span><br><span class="line"><span class="keyword">import</span> scipy <span class="keyword">as</span> sp</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.gridspec <span class="keyword">as</span> gridspec</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'retina'</span></span><br></pre></td></tr></table></figure>
<h3 id="SVC-Support-Vector-Classificaiton-支持向量分类"><a href="#SVC-Support-Vector-Classificaiton-支持向量分类" class="headerlink" title="SVC: Support Vector Classificaiton 支持向量分类"></a>SVC: Support Vector Classificaiton 支持向量分类</h3><p><code>In[2]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC, LinearSVC</span><br></pre></td></tr></table></figure>
<ul>
<li>在libsvm的基础上实现</li>
<li>时间复杂度： more than $O(n^2)$</li>
<li>对于大的样本量（比如多于10000个样本），难以很好的scale</li>
<li>多类分类问题，是基于 OVO（one vs one）的算法</li>
<li>可以实现不同的kernel函数</li>
</ul>
<p><code>In[3]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#linear_svc = LinearSVC()</span></span><br><span class="line">linear_svc = SVC(kernel = <span class="string">"linear"</span>)</span><br><span class="line">print(linear_svc)</span><br></pre></td></tr></table></figure>
<p><code>Out[3]</code></p>
<pre><code>SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto_deprecated&#39;,
  kernel=&#39;linear&#39;, max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
</code></pre><p><code>In[4]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">svc = SVC()</span><br><span class="line">print(svc)</span><br></pre></td></tr></table></figure>
<p><code>Out[4]</code></p>
<pre><code>SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto_deprecated&#39;,
  kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)
</code></pre><ul>
<li>C : 浮点（float），默认值为1.0<script type="math/tex; mode=display">
\begin{align}
& J(\boldsymbol{w}) = \sum_{i=1}^m{\max(1-y_i{\boldsymbol{w}^T\boldsymbol{x}_i, 0)}}+\lambda \|\boldsymbol{w} \|^2 \\
\Rightarrow \ \ & J(\boldsymbol{w}) = C\sum_{i=1}^m{\max(1-y_i{\boldsymbol{w}^T\boldsymbol{x}_i, 0)}}+ \|\boldsymbol{w} \|^2 \\
\end{align}</script></li>
<li><p><strong>C是其他算法里正则化强度 $\lambda$ 的倒数，C越大，正则化强度越小， 容易过拟合</strong></p>
</li>
<li><p>kernel : 和函数，输入值是string,  默认值是 “rbf”（高斯），亦或是</p>
<ol>
<li><p>“linear”： 线性$\ \ k(x_1,x_2) = x_1^Tx_2$  </p>
</li>
<li><p>“poly”： 多项式 $\ k(x_1,x_2) = (x_1^Tx_2 + r)^d $  </p>
</li>
<li><p>“rbf”: 高斯$\ \ \ k(x_1,x_2) = e^{-\frac{|x_1-x_2|^2}{2\sigma^2}} = e^{-\gamma (x_1-x_2)^2}$  </p>
</li>
<li><p>”sigmoid“$\ \ \ \ k(x_1,x_2) = tanh(\beta x_1^Tx_2 + \theta), \beta &gt;0, \theta &lt;0$</p>
</li>
</ol>
</li>
</ul>
<ul>
<li><p>decision function: “ovr“（one vs rest） or ”ovo“（one vs one） 针对于多类分类：</p>
<ul>
<li><strong>ovr： 会有n_labels个分类器</strong></li>
<li><strong>ovo： 会有n_labels(n_labels-1)/2个分类器</strong></li>
</ul>
</li>
</ul>
<p><code>In[5]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons, make_circles, make_classification, make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.gaussian_process.kernels <span class="keyword">import</span> RBF</span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br></pre></td></tr></table></figure>
<p><code>In[6]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X, y = make_classification(n_features = <span class="number">2</span>, n_redundant = <span class="number">0</span>, n_informative = <span class="number">2</span>, n_samples= <span class="number">200</span>,</span><br><span class="line">                           random_state = <span class="number">1</span>, n_clusters_per_class = <span class="number">1</span>)</span><br><span class="line">rng = np.random.RandomState(<span class="number">2</span>)</span><br><span class="line">X += <span class="number">2</span> * rng.uniform(size=X.shape)</span><br></pre></td></tr></table></figure>
<p><code>In[7]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(np.unique(y))</span><br></pre></td></tr></table></figure>
<pre><code>[0 1]
</code></pre><p><code>In[8]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(X.shape)</span><br><span class="line">print(y.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(200, 2)
(200,)
</code></pre><p><code>In[9]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cm = plt.cm.RdBu</span><br><span class="line">cm_bright = ListedColormap([<span class="string">'#FF0000'</span>, <span class="string">'#0000FF'</span>])</span><br></pre></td></tr></table></figure>
<p><code>In[10]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:, <span class="number">1</span>], c = y)</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"x2"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Text(0, 0.5, &#39;x2&#39;)
</code></pre><p><img src="https://i.loli.net/2019/04/21/5cbc310c18910.png" alt="png"></p>
<h4 id="Linear-SVM"><a href="#Linear-SVM" class="headerlink" title="Linear SVM"></a>Linear SVM</h4><p><code>In[11]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 线性核函数的支持向量机去训练样本</span></span><br><span class="line">C = <span class="number">0.1</span></span><br><span class="line">clf = SVC(kernel = <span class="string">"linear"</span>, C = C)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练之前对数据进行normalization</span></span><br><span class="line">X = StandardScaler().fit_transform(X)</span><br><span class="line"></span><br><span class="line">print(X.shape, y.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(200, 2) (200,)
</code></pre><p><code>In[12]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score, f1_score, confusion_matrix</span><br><span class="line"><span class="keyword">from</span> mlxtend.plotting <span class="keyword">import</span> plot_decision_regions</span><br></pre></td></tr></table></figure>
<p><code>In[13]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%%time </span><br><span class="line">clf.fit(X, y)</span><br><span class="line">y_pred = clf.predict(X)</span><br></pre></td></tr></table></figure>
<pre><code>CPU times: user 2.32 ms, sys: 1.58 ms, total: 3.9 ms
Wall time: 4.46 ms
</code></pre><p><code>In[14]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">prec = precision_score(y_true = y, y_pred = y_pred, pos_label = <span class="number">1</span>)</span><br><span class="line">rec = recall_score(y_true = y, y_pred = y_pred, pos_label = <span class="number">1</span>)</span><br><span class="line">f1 = f1_score(y_true = y, y_pred = y_pred, pos_label = <span class="number">1</span>)</span><br><span class="line">print(<span class="string">"Precision score is : &#123;:.2f&#125;"</span>.format(prec))</span><br><span class="line">print(<span class="string">"Recall score is : &#123;:.2f&#125;"</span>.format(rec))</span><br><span class="line">print(<span class="string">"f1 score is : &#123;:.2f&#125;"</span>.format(f1))</span><br><span class="line">print(<span class="string">"Confusion matrix is :"</span>) </span><br><span class="line">print(confusion_matrix(y_pred = y_pred, y_true = y))</span><br></pre></td></tr></table></figure>
<pre><code>Precision score is : 0.88
Recall score is : 0.85
f1 score is : 0.87
Confusion matrix is :
[[90 11]
 [15 84]]
</code></pre><p><code>In[15]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plot_decision_regions(X, y, clf = clf, colors = <span class="string">'orange,navy'</span>)</span><br><span class="line">plt.title(<span class="string">"SVM with linear kernel"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/04/21/5cbc310bf2d06.png" alt="png"></p>
<h4 id="SVM-with-rbf-kernel"><a href="#SVM-with-rbf-kernel" class="headerlink" title="SVM with rbf kernel"></a>SVM with rbf kernel</h4><p><code>In[16]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">C = <span class="number">0.1</span></span><br><span class="line"><span class="comment">#clf = SVC(kernel = "rbf", gamma = 2., C = C)</span></span><br><span class="line">clf = SVC(gamma = <span class="number">2.</span>, C = C)</span><br><span class="line"></span><br><span class="line">clf.fit(X, y)</span><br><span class="line">y_pred = clf.predict(X)</span><br><span class="line"></span><br><span class="line">prec = precision_score(y_true = y, y_pred = y_pred, pos_label = <span class="number">1</span>)</span><br><span class="line">rec = recall_score(y_true = y, y_pred = y_pred, pos_label = <span class="number">1</span>)</span><br><span class="line">f1 = f1_score(y_true = y, y_pred = y_pred, pos_label = <span class="number">1</span>)</span><br><span class="line">print(<span class="string">"Precision score is : &#123;:.2f&#125;"</span>.format(prec))</span><br><span class="line">print(<span class="string">"Recall score is : &#123;:.2f&#125;"</span>.format(rec))</span><br><span class="line">print(<span class="string">"f1 score is : &#123;:.2f&#125;"</span>.format(f1))</span><br><span class="line">print(<span class="string">"Confusion matrix is :"</span>) </span><br><span class="line">print(confusion_matrix(y_pred = y_pred, y_true = y))</span><br><span class="line"></span><br><span class="line">plot_decision_regions(X, y, clf = clf, colors = <span class="string">'orange,navy'</span>)</span><br><span class="line">plt.title(<span class="string">"SVM with rbf kernel"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>Precision score is : 0.85
Recall score is : 0.86
f1 score is : 0.85
Confusion matrix is :
[[86 15]
 [14 85]]
</code></pre><p><img src="https://i.loli.net/2019/04/21/5cbc310bf38ed.png" alt="png"></p>
<p>接下来定义不同参数，对比不同的结果</p>
<p><code>In[17]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">C_list = [<span class="number">0.1</span>, <span class="number">1.</span>, <span class="number">10.</span>]</span><br><span class="line">gamma_list = [<span class="number">0.2</span>, <span class="number">2.</span>, <span class="number">20.</span>]</span><br></pre></td></tr></table></figure>
<h5 id="SVM-高斯核函数参数1-C"><a href="#SVM-高斯核函数参数1-C" class="headerlink" title="SVM 高斯核函数参数1: C"></a>SVM 高斯核函数参数1: C</h5><ul>
<li>其中 C是惩罚系数，即对误差的宽容度</li>
<li>C 越小， 正则化强度越大，容易欠拟合</li>
<li>C 越大， 说明越不能容忍出现误差,容易过拟合</li>
</ul>
<p><code>In[18]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize = (<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i, C <span class="keyword">in</span> enumerate(C_list):</span><br><span class="line">    clf = SVC(C = C)</span><br><span class="line">    clf.fit(X, y)</span><br><span class="line">    <span class="comment">#y_pred = clf.predict(X)</span></span><br><span class="line">    plt.subplot(<span class="number">1</span>, len(C_list), i + <span class="number">1</span>)</span><br><span class="line">    plot_decision_regions(X, y, clf = clf)</span><br><span class="line">    plt.title(<span class="string">"SVM with rbf kernel, C = &#123;:.4f&#125;"</span>.format(C))</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/04/21/5cbc310c26621.png" alt="png"></p>
<h5 id="SVM-高斯核函数参数2-gamma"><a href="#SVM-高斯核函数参数2-gamma" class="headerlink" title="SVM 高斯核函数参数2: gamma"></a>SVM 高斯核函数参数2: gamma</h5><ul>
<li>gamma是选择RBF函数作为kernel后，该函数自带的一个参数</li>
<li>隐含地决定了数据映射到新的特征空间后的分布</li>
<li>gamma越大，单个样本对整个分类超平面的影响比较大，整个模型的支持向量也会多， 过拟合</li>
<li>gamma值越小，单个样本对整个分类超平面的影响比较小，不容易被选择为支持向量，容易欠拟合<script type="math/tex; mode=display">
k(x_1,x_2) = e^{-\frac{|x_1-x_2|^2}{2\sigma^2}} = e^{-\gamma (x_1-x_2)^2}</script></li>
</ul>
<p><code>In[19]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">gamma_list = [<span class="number">0.05</span>, <span class="number">2.</span>, <span class="number">20.</span>]</span><br><span class="line">plt.figure(figsize = (<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i, gamma <span class="keyword">in</span> enumerate(gamma_list):</span><br><span class="line">    clf = SVC(gamma = gamma, C = <span class="number">1.0</span>)</span><br><span class="line">    clf.fit(X, y)</span><br><span class="line">    <span class="comment">#y_pred = clf.predict(X)</span></span><br><span class="line">    plt.subplot(<span class="number">1</span>, len(C_list), i + <span class="number">1</span>)</span><br><span class="line">    plot_decision_regions(X, y, clf = clf)</span><br><span class="line">    plt.title(<span class="string">"SVM with rbf kernel, gamma = &#123;:.2f&#125;"</span>.format(gamma))</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/04/21/5cbc310c26fd3.png" alt="png"></p>
<h4 id="不同SVM核函数对比"><a href="#不同SVM核函数对比" class="headerlink" title="不同SVM核函数对比"></a>不同SVM核函数对比</h4><p><code>In[20]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">names = [ <span class="string">"Linear SVM"</span>, <span class="string">"rbf SVM"</span>, <span class="string">"Poly SVM"</span>, <span class="string">"Sigmoid SVM"</span>]</span><br><span class="line"></span><br><span class="line">models = [</span><br><span class="line">            SVC(kernel = <span class="string">"linear"</span>, C = C), </span><br><span class="line">            SVC(kernel = <span class="string">"rbf"</span>, gamma = <span class="number">2</span>, C = C), <span class="comment"># or SVC(gamma = 2, C = 1)</span></span><br><span class="line">            SVC(kernel = <span class="string">"poly"</span>, degree = <span class="number">5</span>, C = C),</span><br><span class="line">            SVC(kernel = <span class="string">"sigmoid"</span>, C = C)</span><br><span class="line">         ]</span><br><span class="line"></span><br><span class="line">figure = plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line"><span class="keyword">assert</span> len(names) == len(models)</span><br><span class="line"><span class="keyword">for</span> i, (clf_name, clf) <span class="keyword">in</span> enumerate(zip(names, models)):</span><br><span class="line">    plt.subplot((len(models)+<span class="number">1</span>)/<span class="number">2</span>, <span class="number">2</span>, i + <span class="number">1</span>)</span><br><span class="line">    clf.fit(X, y)</span><br><span class="line">    plot_decision_regions(X, y, clf)</span><br><span class="line">    plt.title(clf_name, fontsize = <span class="number">12</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/04/21/5cbc310c31cb7.png" alt="png"></p>
<h4 id="多类分类：multiclass-classficiation"><a href="#多类分类：multiclass-classficiation" class="headerlink" title="多类分类：multiclass classficiation"></a>多类分类：multiclass classficiation</h4><ul>
<li>OVO</li>
<li>OVR</li>
</ul>
<p><code>In[21]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsOneClassifier, OneVsRestClassifier</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X2, Y2 = make_blobs(n_samples=<span class="number">200</span>, n_features = <span class="number">2</span>, centers = <span class="number">5</span>, random_state=<span class="number">1</span>,cluster_std = <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><code>In[22]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(np.unique(Y2))</span><br></pre></td></tr></table></figure>
<pre><code>[0 1 2 3 4]
</code></pre><p><code>In[23]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X2[:,<span class="number">0</span>], X2[:,<span class="number">1</span>], c = Y2)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/04/21/5cbc310c1a556.png" alt="png"></p>
<p><code>In[24]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rbf_ovr = SVC(kernel = <span class="string">'rbf'</span>, decision_function_shape = <span class="string">"ovr"</span>, gamma=<span class="string">'auto'</span>)</span><br><span class="line"><span class="comment">#rbf_ovo = SVC(kernel = 'rbf', decision_function_shape = "ovo")</span></span><br><span class="line">rbf_ovo = OneVsOneClassifier(SVC(kernel = <span class="string">"rbf"</span>, gamma=<span class="string">'auto'</span>))</span><br><span class="line">linear_ovr = SVC(kernel = <span class="string">'linear'</span>, decision_function_shape = <span class="string">"ovr"</span>, gamma=<span class="string">'auto'</span>)</span><br><span class="line"><span class="comment">#linear_ovo = SVC(kernel = 'linear', decision_function_shape = "ovo")</span></span><br><span class="line">linear_ovo = OneVsOneClassifier(SVC(kernel = <span class="string">"linear"</span>))</span><br></pre></td></tr></table></figure>
<p><code>In[25]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">gs = gridspec.GridSpec(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">fig = plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">clfs = [rbf_ovr, rbf_ovo, linear_ovr, linear_ovo]</span><br><span class="line">names = [<span class="string">"RBF SVM OVR"</span>, <span class="string">"RBF SVM OVO"</span>, <span class="string">"Linear SVM OVR"</span>, <span class="string">"Linear SVM OVO"</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> clf, lab, grd <span class="keyword">in</span> zip(clfs, names, itertools.product([<span class="number">0</span>, <span class="number">1</span>], repeat=<span class="number">2</span>)):</span><br><span class="line">    ax = plt.subplot(gs[grd[<span class="number">0</span>], grd[<span class="number">1</span>]])</span><br><span class="line">    clf.fit(X2, Y2)</span><br><span class="line">    fig = plot_decision_regions(X=X2, y=Y2, clf=clf, legend=<span class="number">2</span>)</span><br><span class="line">    plt.title(lab)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/04/21/5cbc310c41c4c.png" alt="png"></p>
<h3 id="SVR-Support-Vector-Regression-支持向量回归"><a href="#SVR-Support-Vector-Regression-支持向量回归" class="headerlink" title="SVR: Support Vector Regression 支持向量回归"></a>SVR: Support Vector Regression 支持向量回归</h3><p><code>In[26]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVR</span><br></pre></td></tr></table></figure>
<p><code>In[27]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据</span></span><br><span class="line">X = np.sort(<span class="number">5</span> * np.random.rand(<span class="number">40</span>, <span class="number">1</span>), axis=<span class="number">0</span>)</span><br><span class="line">y = np.sin(X).ravel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 人为增加一些噪声</span></span><br><span class="line">y[::<span class="number">5</span>] += <span class="number">3</span> * (<span class="number">0.5</span> - np.random.rand(<span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">plt.scatter(X, y)</span><br><span class="line">plt.xlabel(<span class="string">"X"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"y"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/04/21/5cbc310bd8e38.png" alt="png"></p>
<p><code>In[28]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SVR 拟合数据</span></span><br><span class="line">C = <span class="number">1e3</span></span><br><span class="line">svr_rbf = SVR(kernel=<span class="string">'rbf'</span>, C = C, gamma = <span class="number">0.1</span>)</span><br><span class="line">svr_lin = SVR(kernel=<span class="string">'linear'</span>, C = C)</span><br><span class="line">svr_poly = SVR(kernel=<span class="string">'poly'</span>, C = C, degree = <span class="number">2</span>, gamma=<span class="string">'auto'</span>)</span><br><span class="line">y_rbf = svr_rbf.fit(X, y).predict(X)</span><br><span class="line">y_lin = svr_lin.fit(X, y).predict(X)</span><br><span class="line">y_poly = svr_poly.fit(X, y).predict(X)</span><br><span class="line"></span><br><span class="line">lw = <span class="number">4</span></span><br><span class="line">plt.figure(figsize = (<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line">plt.scatter(X, y, color=<span class="string">'darkorange'</span>, label=<span class="string">'data'</span>)</span><br><span class="line">plt.plot(X, y_rbf, color=<span class="string">'navy'</span>, lw = lw, label=<span class="string">'RBF model'</span>)</span><br><span class="line">plt.plot(X, y_lin, color=<span class="string">'c'</span>, lw = lw, label=<span class="string">'Linear model'</span>)</span><br><span class="line">plt.plot(X, y_poly, color=<span class="string">'cornflowerblue'</span>, lw=lw, label=<span class="string">'Polynomial model'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'data'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'target'</span>)</span><br><span class="line">plt.title(<span class="string">'Support Vector Regression'</span>)</span><br><span class="line">plt.legend(loc = <span class="string">'lower left'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/04/21/5cbc310c1e34e.png" alt="png"></p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>Gavin
  </li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="https://marara.xin/2019/04/21/SVM/" title="支持向量机（SVM）">https://marara.xin/2019/04/21/SVM/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>

        

  <div class="followme">
    <p>Welcome to my other publishing channels</p>

    <div class="social-list">

            <div class="social-item">
              <a target="_blank" class="social-link" href="https://twitter.com/rp_Wang">
                <span class="icon">
                  <i class="fa fa-twitter"></i>
                </span>

                <span class="label">Twitter</span>
              </a>
            </div>

            <div class="social-item">
              <a target="_blank" class="social-link" href="/atom.xml">
                <span class="icon">
                  <i class="fa fa-rss"></i>
                </span>

                <span class="label">RSS</span>
              </a>
            </div>
    </div>
  </div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/Statistics/" rel="tag"><i class="fa fa-tag"></i> Statistics</a>
              <a href="/tags/Machine-Learning/" rel="tag"><i class="fa fa-tag"></i> Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/04/20/推荐系统中的NDCG/" rel="prev" title="推荐系统中的NDCG">
      <i class="fa fa-chevron-left"></i> 推荐系统中的NDCG
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#简介"><span class="nav-text">简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#机器学习算法分类"><span class="nav-text">机器学习算法分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性分类"><span class="nav-text">线性分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#如何决定最好的参数：-𝑤-amp-𝑏"><span class="nav-text">如何决定最好的参数：$𝑤$ &amp; $𝑏$</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#凸函数"><span class="nav-text">凸函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#凸优化"><span class="nav-text">凸优化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#拉格朗日对偶"><span class="nav-text">拉格朗日对偶</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#拉格朗日乘子：等式约束"><span class="nav-text">拉格朗日乘子：等式约束</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#拉格朗日乘子：不等式约束"><span class="nav-text">拉格朗日乘子：不等式约束</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#拉格朗日对偶-1"><span class="nav-text">拉格朗日对偶</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#支持向量机的对偶问题"><span class="nav-text">支持向量机的对偶问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#核函数（Kernel）"><span class="nav-text">核函数（Kernel）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#核函数"><span class="nav-text">核函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#正则化与软间隔"><span class="nav-text">正则化与软间隔</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#软间隔支持向量机"><span class="nav-text">软间隔支持向量机</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#支持向量机分类与回归"><span class="nav-text">支持向量机分类与回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#支持向量机回归分析（SVR）"><span class="nav-text">支持向量机回归分析（SVR）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SVR-对偶问题"><span class="nav-text">SVR 对偶问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#支持向量机算法总结"><span class="nav-text">支持向量机算法总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#代码实例"><span class="nav-text">代码实例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SVC-Support-Vector-Classificaiton-支持向量分类"><span class="nav-text">SVC: Support Vector Classificaiton 支持向量分类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear-SVM"><span class="nav-text">Linear SVM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM-with-rbf-kernel"><span class="nav-text">SVM with rbf kernel</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#SVM-高斯核函数参数1-C"><span class="nav-text">SVM 高斯核函数参数1: C</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SVM-高斯核函数参数2-gamma"><span class="nav-text">SVM 高斯核函数参数2: gamma</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#不同SVM核函数对比"><span class="nav-text">不同SVM核函数对比</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#多类分类：multiclass-classficiation"><span class="nav-text">多类分类：multiclass classficiation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SVR-Support-Vector-Regression-支持向量回归"><span class="nav-text">SVR: Support Vector Regression 支持向量回归</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Gavin"
      src="/images/IMG_5946.png">
  <p class="site-author-name" itemprop="name">Gavin</p>
  <div class="site-description" itemprop="description">Make it, Fake it.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">23</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Wanguy" title="Github → https://github.com/Wanguy" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>Github</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:w020283852@gmail.com" title="E-Mail → mailto:w020283852@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/rp_Wang" title="Twitter → https://twitter.com/rp_Wang" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.instagram.com/sephirothgavin/" title="Instagram → https://www.instagram.com/sephirothgavin/" rel="noopener" target="_blank"><i class="fa fa-fw fa-instagram"></i>Instagram</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2015 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-bug"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Gavin</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/muse.js"></script>
<script src="/js/next-boot.js"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  <script src="/js/local-search.js"></script>








<script>
if (document.querySelectorAll('div.pdf').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/pdfobject@2/pdfobject.min.js', () => {
    document.querySelectorAll('div.pdf').forEach(element => {
      PDFObject.embed(element.getAttribute('target'), element, {
        pdfOpenParams: {
          navpanes : 0,
          toolbar  : 0,
          statusbar: 0,
          pagemode : 'thumbs',
          view     : 'FitH'
        },
        PDFJS_URL: '/lib/pdf/web/viewer.html',
        height   : element.getAttribute('height') || '500px'
      });
    });
  }, window.PDFObject);
}
</script>




  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '85ce279610ff27b07d1c',
      clientSecret: '9c42386ae5260c368961a8b4481eb48d72e5448e',
      repo        : 'Wanguy.github.io',
      owner       : 'Wanguy',
      admin       : ['Wanguy'],
      id          : '3ad84067a94898d8443fe77e92d86dba',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
